<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Detecting and Tackling Biases in Data Preprocessing for Responsible AI</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            background-color: #f0f0f0;
        }

        header {
            background-color: #282c34;
            color: #61dafb;
            text-align: center;
            padding: 3px 0;
            
        }

        nav {
            background-color: #383e4a;
            padding: 10px;
            margin-bottom: 10px;
            text-align: center;
        }

        nav a {
            color: #61dafb;
            text-decoration: none;
            padding: 10px 20px;
            margin: 0 10px;
            border-radius: 5px;
            transition: background-color 0.3s;
        }

        nav a:hover {
            background-color: #61dafb;
            color: #fff;
        }

        main {
            max-width: 1000px;
            margin: 20px auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }

        section {
            margin-bottom: 30px;
        }

        h2 {
            color: #61dafb;
        }

        img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            margin-top: 10px;
        }

        video {
            max-width: 100%;
            border-radius: 5px;
            margin-top: 10px;
        }
       
        section {
            margin-bottom: 30px;
        }

        h2 {
            color: #61dafb;
        }

        p {
            line-height: 1.6;
            margin-bottom: 15px;
        }
        #team-members {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-around;
            margin-top: 10px;
        }
        .team-member {
            width: 300px;
            margin: 5px;
            padding: 5px;
            text-align: center;
        margin-bottom: 10px;
            transition: transform 0.3s;
        }
        .team-member:hover {
            transform: scale(1.05);
        }
        .team-member h3 {
            color: #61dafb;

        }

       
    </style>
</head>
<body>
    <header>
        <h1>Detecting and Tackling Biases in Data Preprocessing for Responsible AI</h1>
    
    <section id="team-members">
        <div class="team-member">
            <h3>BRITTNEY LAUREN C</h3>
        </div>

        <div class="team-member">
            <h3>KAMALESHWARI T</h3>
        </div>

        <div class="team-member">
            <h3>PRATHIYUSHA N</h3>
        </div>
    </section>
    </header>
    <nav>
        <a href="#abstract">Abstract</a>
        <a href="#methods">Methods</a>
        <a href="#proposal">Proposal</a>
        <a href="#images">Images</a>
        <a href="#video">Video Demo</a>
    </nav>

    

    <main>
        <section id="abstract">
            <h2>Abstract</h2>
            <p>This project aims to address the challenge of detecting and mitigating biases in data preprocessing using responsible AI tools. Understanding common biases in different stages of AI development and ensuring a deep understanding of the data and domain expertise are crucial to avoid biases. Additionally, avoiding sensitive groups like gender, socioeconomic position, and ethnic traits is essential to reduce bias in data preprocessing. This highlights the importance of addressing biases in data preprocessing for responsible AI and outlines the methods and considerations involved in detecting and mitigating biases in AI systems.</p>
        </section>

        
            <section id="proposal">
                <h2>Proposal</h2>
                <p>In the realm of artificial intelligence, the quality and integrity of data significantly influence the performance and fairness of AI models. One critical stage in AI development is data preprocessing, where raw data is cleaned, transformed, and prepared for model training. However, biases present in the data or introduced during preprocessing can lead to skewed or unfair outcomes, raising serious ethical and practical concerns. This project aims to address the challenge of “Detecting and Tackling Biases in Data Preprocessing for Responsible AI”</p>
                <h3>Bias Identification and Analysis:</h3><p> Develop methods to identify biases in both the raw and preprocessed data, considering various types of biases such as demographic, cultural or contextual biases.
                Analyze the impact of biases on model performance, fairness, and the potential to perpetuate or mitigate existing disparities. </p>
                <p>Present your proposal or idea in detail. Outline the problem you aim to solve, your proposed solution, and the benefits it brings. This section is crucial for conveying the purpose and value of your product or service. You may include charts, graphs, or visual aids to enhance the understanding of your proposal.</p>
                
                <h3>Bias Mitigation Techniques in Preprocessing:</h3><p> Research and implement preprocessing techniques that can help mitigate identified biases. Ensuring fairness and equity in the resulting AI models.</p>
                    <p>Explore methods to re-balance the data, adjust weights, or modify features to minimize biases without compromising the utility of the data.</p>
           
                    <h3>Robust Data Imputation Strategies:</h3><p> Develop strategies for handling missing or incomplete data in a way that avoids introducing biases during imputation, ensuring a representative and fair dataset. </p>
                        
                        <h3>Ethical and Legel Compliance:</h3><p> Ensure that data preprocessing techniques align with ethical and legal standards, respecting privacy, consent, and other regulatory requirements during the handling of sensitive data. </p>
                            
                            <h3>Automated Bias Detection and Correction:</h3>
                                <p>Investigate the feasibility of automation in detecting biases during data preprocessing and incorporating automated correction mechanisms to streamline the bias mitigation process.</p>
                                <h3>Performance Evaluation and Benchmarking:</h3><p>Establish metrics and evaluation procedures to measure the effectiveness of bias detection and mitigation techniques in data preprocessing.  </p>
                                <p>Benchmark the performance of the AI models after applying the bias mitigation techniques against industry standards and best practices.</p>
                        </section>
    

                        <section id="images">
                            <h2>Images</h2>
                            <h3>Level 1: System Context Diagram</h3><center><img src="Level 1.jpeg" alt="Level 1"></center>
                            <h3>Level 2: Container Diagram</h3> <center><img src="Level 2.jpeg" alt="Level 2"></center>
                            <h3>Level 3: Component Diagram</h3> <center><img src="Level 3.jpeg" alt="Level 3"></center>
                            <h3>Level 4: Code Diagram</h3><img src="Level 4.jpeg" alt="Level 4">
                        </section>
                
                        <section id="methods">
                            <h2>Methods</h2>
                            <h3>Selection Bias</h3>
                            <p>The Selection Bias occurs when the data used to train a machine learning model is not representative of the population it aims to generalize to. This can lead to skewed or inaccurate predictions. Detecting selection bias in data preprocessing is crucial for ensuring the fairness and accuracy of AI models. The method to detect selection bias is to test the model's predictive performance. If the model's performance varies significantly between different subsets of the data, it may indicate the presence of selection bias.</p>
                            <h3>Dataset Information:</h3>
                            <p>1. <a href="https://archive.ics.uci.edu/dataset/2/adult" target="_blank">Adult</a></p>
                            <p>2. <a href="https://www.kaggle.com/datasets/kukuroo3/body-performance-data" target="_blank">BodyPerformance</a></p>
                            <p>3. <a href="https://www.kaggle.com/datasets/vishwas199728/credit-card?select=Credit_card.csv" target="_blank">Credit Score</a></p>
                            <p>4. <a href="https://archive.ics.uci.edu/dataset/183/communities+and+crime" target="_blank">Crime</a></p>
                            <p>5. <a href="https://www.kaggle.com/code/mohamedharris/detailed-analysis-prediction-of-heart-failure/input" target="_blank">Heart Failure</a></p>
                            <p>6. <a href="https://www.kaggle.com/code/bbhatt001/predictors-of-medical-expenses/input" target="_blank">Insurance</a></p>
                            <p>7. <a href="https://www.kaggle.com/datasets/danofer/law-school-admissions-bar-passage" target="_blank">LawSchool</a></p>
                            <p>8. <a href="https://www.kaggle.com/code/alankarmahajan/exploring-spotify-dataset" target="_blank">Spotify</a></p>
                            <p>9. <a href="https://www.kaggle.com/code/ahmedali058/student-grade-prediction-datascience-project/input" target="_blank">Student</a></p>
                            <p>10. <a href="https://www.kaggle.com/datasets/brendan45774/test-file" target="_blank">Titanic</a></p>
                            
            <h3>Bias Measurement methods:</h3>
            <p><strong>Statistical parity: </strong>Computes the difference in success rates between the protected
groups. Values below 0 are considered unfair towards group_a while values above 0 are
considered unfair towards group_b, the range (-0.1, 0.1) is considered acceptable.</p>
<p><strong>Disparate Impact: </strong>Shows the ratio of success rates between the protected groups for a
certain quantile. Values below 1 are unfair towards group_a. Values above 1 are unfair
towards group_b. The range (0.8, 1.2) is considered acceptable.</p>
<p><strong>Four Fifths:</strong>Computes the ratio of success rates between the protected groups. Values
below 1 are considered unfair while a range between (0.8, 1) is considered acceptable.</p>
<p><strong>Cohen D: </strong>Computes the normalised statistical parity between the protected groups.
Values below 0 are considered unfair towards group_a while values above 0 are
considered unfair towards group_b.</p>
<p><strong>Equality of opportunity difference: </strong>Computes the difference in true positive rates
between the protected groups. Values below 0 are considered unfair towards group_a
while values above 0 are considered unfair towards group_b.</p>
<p><strong>False positive rate difference: </strong>Computes the difference in false positive rates between
the protected groups, negative values indicating bias against group_a while positive
values indicating bias against group_b.</p>
<p><strong>Average Odds Difference: </strong>Computes the difference in average odds between the
protected groups, negative values indicating bias against group_a while positive values
indicating bias against group_b, a range between (-0.1, 0.1) is considered acceptable.</p>
<p><strong>Accuracy Difference: </strong>Computes the difference in accuracy of predictions for the
protected groups, positive values show bias against group_b while negative values show
bias against group_a.</p>
<p><strong>classification_bias_metrics: </strong>function allows us to select which metrics we want to
calculate, if equal_outcome, equal_opportunity or both, where equal_outcome shows
how disadvantaged groups are treated by the model and equal_opportunity shows if all
the groups have the same opportunities.</p>
<p><strong>multiclass_bias_metrics: </strong>function allows us to select which metrics we want to
calculate, if equal_outcome, equal_opportunity or both, where equal_outcome shows
how disadvantaged groups are treated by the model and equal_opportunity shows if all
the groups have the same opportunities.</p>
<p><strong>Multiclass Statistical parity: </strong>Computes the statistical parity between multiple classes
and a protected attribute with multiple groups. For each group it computes the vector of
success rates for entering each class, finally uses the mean or max strategies to
aggregate them. Same as the 1d case, values in the range (-0.1, 0.1) are considered
acceptable.</p>
<p><strong>Multiclass Equality of Opportunity: </strong>Computes the matrix of error rates for each group,
then computes all distances (mean absolute deviation) between such matrices, finally
uses the mean or max strategies to aggregate them. Same as the 1d case, values in the
range (-0.1, 0.1) are considered acceptable.</p>
<p><strong>Multiclass Average Odds: </strong>Computes the matrix of error rates for each group, then
averages these matrices over rows, and computes all pairwise distances between the
resulting vectors, finally uses the mean or max strategies to aggregate them. Same as
the 1d case, values in the range (-0.1, 0.1) are considered acceptable.</p>
<p><strong>Multiclass True Positive Difference: </strong>Computes the matrix of error rates for each
group, then computes all pairwise distances between the diagonal of such matrices,
finally uses the mean or max strategies to aggregate them. Same as the 1d case, values
in the range (-0.1, 0.1) are considered acceptable.</p>
<p><strong>Average score difference: </strong>Computes the difference in average scores between the
protected groups. Negative values indicate the group_a has lower average score, so
bias against group_a, while positive values indicate group_b has lower average score,
so bias against group_b.</p>
<p><strong>Z score difference: </strong>Computes the spread in Z Scores between the protected groups,
the Z Score is a normalised version of Disparate Impact.</p>
<p><strong>Max Statistical Parity: </strong>Computes the maximum over all thresholds of the absolute
statistical parity between the protected groups, values below 0.1 in absolute value are
considered acceptable.</p>
<p><strong>RMSE ratio: </strong>Computes the RMSE for the protected groups, lower values show bias
against group_a while higher values show bias against group_b.</p>
<p><strong>MAE ratio: </strong>Similar to the previous metric, computes the MAE for the protected groups,
lower values show bias against group_a while higher values show bias against group_b.</p>
<p><strong>Correlation difference: </strong>Computes the difference in correlation between predictions and
targets for the protected groups, positive values show bias against group_a while
negative values show bias against group_b.</p>
<p><strong>clustering_bias_metrics: </strong>function allows us to select which metrics we want to
calculate, if equal_outcome, equal_opportunity or both, where equal_outcome shows
how disadvantaged groups are treated by the model and equal_opportunity shows if all
the groups have the same opportunities.</p>
<p><strong>Cluster Balance: </strong>Given a clustering and protected attribute. The cluster balance is the
minimum over all groups and clusters of the ratio of the representation of members of
that group in that cluster to the representation overall. A value of 1 is desired. That is
when all clusters have the exact same representation as the data. Lower values imply
the existence of clusters where either group_a or group_b is underrepresented.</p>
<p><strong>Minimum Cluster Ratio: </strong>Given a clustering and protected attributes. The min cluster
ratio is the minimum over all clusters of the ratio of number of group_a members to the
number of group_b members. A value of 1 is desired. That is when all clusters are
perfectly balanced. Low values imply the existence of clusters where group_a has fewer
members than group_b.</p>
<p><strong>Cluster Distribution Total Variation: </strong>This function computes the distribution of group_a
and group_b across clusters. It then outputs the total variation distance between these
distributions. A value of 0 is desired. That indicates that both groups are distributed
similarly amongst the clusters. The metric ranges between 0 and 1, with higher values
indicating the groups are distributed in very different ways.</p>
<p><strong>Cluster Distribution KL Div: </strong>This function computes the distribution of group_a and
group_b membership across the clusters. It then returns the KL distance from the
distribution of group_a to the distribution of group_b. A value of 0 is desired. That
indicates that both groups are distributed similarly amongst the clusters. Higher values
indicate the distributions of both groups amongst the clusters differ more.</p>
<p><strong>Social Fairness Ratio: </strong>Given a centroid based clustering, this function compute the
average distance to the nearest centroid for both groups. The metric is the ratio of the
resulting distance for group_a to group_b. A value of 1 is desired. Lower values indicate
the group_a is on average closer to the respective centroids. Higher values indicate that
group_a is on average further from the respective centroids.</p>
<p><strong>Silhouette Difference: </strong>We compute the difference of the mean silhouette score for both
groups. The silhouette difference ranges from -1 to 1, with lower values indicating bias
towards group_a and larger values indicating bias against group_b.</p>
<p><strong>Exposure ratio: </strong>Calculates the relation between the exposure of non-protected and
protected elements from the dataset. For a fairer model we seek to have this value
lower, indicating that the protected examples are gaining more exposure.</p>
<p><strong>Exposure difference: </strong>Calculates the difference of exposure between the two groups
this value will be zero when the protected group achieves more exposure than the
non-protected.</p>
<h3>Bias Mitigation methods:</h3>
<p><strong>Correlation remover: </strong>is a pre-processing technique that applies a linear transformation
to the non-sensitive features of the dataset to remove the correlation with respect to the
sensitive columns. This process is done aiming to maintain as much as possible to
prevent lost information</p>
<ul><li><strong>alpha </strong>- parameter to control how much to filter, for alpha=1.0 we filter out all
information while for alpha=0.0 we don’t apply any.</li></ul>
<p><strong>Disparate impact remover:</strong>is a pre-processing technique that uses perturbation to
modify the values of the features such that the distributions of privileged and
unprivileged groups are close in order to increase fairness.</p>
<ul><li><strong>repair_level value</strong>- parameter to control the repair amount, where 0 means no
repair while 1 is full repair.</li></ul>
<p><strong>Reweighing: </strong>is an pre-processing technique that adapts the impact of the training
instances by reweighing their importance according to its label and the protected
attributes to ensure fairness before classification.</p>
<p><strong>Disparate impact remover RS: </strong>is a preprocessing algorithm that edits feature values to
increase group fairness while preserving rank-ordering within groups.</p>
<h3>Graphs and Metrics:</h3>
<p><strong>Method: CorrelationRemover <br>Binary Classification:</strong></p>
<center><img src="bc_1.jpg" alt=""><img src="bc_2.jpg" alt=""></center>
<p><strong>Multi Classification: </strong></p>
<center><img src="mc_1.jpg" alt=""><img src="mc_2.jpg" alt=""></center>
<p><strong>Regression:</strong></p>
<center><img src="reg_1.jpg" alt=""><img src="reg_2.jpg" alt=""></center>

<p><strong></strong></p>
            
            <h3>Labeling Bias</h3>
            <p>Label bias occurs when the labels assigned to the data are incorrect or subjective, leading to an unfair model. This bias can result from various sources, such as human bias in historical data, correlated features, or proxies. Detecting label bias in data preprocessing is crucial for ensuring the fairness and accuracy of AI models. To detecting label bias is to analyze the data and examine the dataset for potential biases. This can include inspecting the data source, monitoring the timeframe of data collection, and confirming any potential prejudice associated with the dataset.</p>

        </section>
        <h3>Dataset Information:</h3>
        <p>This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases</p>
        <p><a href="https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset" target="_blank">Diabtes Dataset</a></p>

            <p><strong>Supervised Classifier:</strong>In supervised learning, the model is trained on a labelled dataset, where each data point is associated with a correct label. Human annotators provide the labels for the training data, and the model learns to make predictions based on this labelled information. Labelling bias in supervised learning can occur if the training data is biased or if the labelling process introduces bias.</p>

            <p><strong>Semi-Supervised Classifier:</strong>Semi-supervised learning involves a combination of labelled and unlabelled data for training. A smaller portion of the dataset is labelled, and the model learns from both the labelled and unlabelled examples. In the context of labelling bias, semi-supervised learning can help mitigate bias by leveraging a larger pool of unlabelled data, which may be less biased compared to the labelled subset.</p>

            <p><strong>Unsupervised Classifier: </strong>Unsupervised learning is used when the training data is not labelled, and the algorithm must find patterns or structure in the data without explicit guidance. Since there are no labelled examples, biases introduced through the labelling process are not a concern in unsupervised learning. However, biases may still exist in the data itself, and unsupervised learning methods can inadvertently learn and propagate these biases.</p>

            <p><strong>Output: </strong>
                <h3>Supervised Classifier:</h3><center><img src="L1.png" alt="L1"></center>
                <h3>Semi-Supervised Classifier:</h3><center><img src="L2.png" alt="L2"></center>
                <h3>Unsupervised Classifier:</h3><center><img src="L3.png" alt="L3"></center>
                
                <p><h2>Conclusion</h2>
                    <p><strong>Selection Bias: </strong>
<p>As we can see from the CorrelationRemover method, we are able to get closer to the reference
when we increase the alpha parameter, but we need to keep in mind that, as the alpha
parameter value increases, the information filtered will be higher, but accuracy will also be
diminished. This also applies to the other methods. In the case of a proportional decrease
between accuracy and fairness, the dataset is already almost fair. <br> The Choice of model parameters depends on our main objective, whether fairness or accuracy is our goal.</p>
                <p><strong>Labelling Bias:</strong> <br/><br/>The dataset initially exhibits class imbalance, and efforts have been made to address this issue by adjusting class weights. Three different classifiers (supervised, semi-supervised, and unsupervised) have been employed, each with its own set of class weights, indicating an exploration of different strategies to handle imbalanced data. Clustering has also been performed, leading to the identification of four unique clusters.</p>
              
<p><strong>Github Page Link:</strong></p>    
<p><a href="https://github.com/RincisM/Responsive_AI/tree/main/Data_Preprocessing" target="_blank">Click Here</a></p>

        <section id="video">
            <h2>Video Demo</h2>
            <video controls>
                <source src="Data_Preprocessing.mov">
            </video>
        </section>
    </main>
</body>
</html>
